import numpy as np
import cv2
import torch
import time
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
# Image Input
directory_name = 'E/color_7_9'
Image = np.zeros([250,224,224,3])
img = cv2.imread('E/color_7_9/100.jpg')
img_shape = img.shape
new_img = img[int(img_shape[0]/10*4):int(img_shape[0]/10*6), int(img_shape[1]/10*4):int(img_shape[1]/10*6)]

start_time = time.time()  # time
# training dataset
for i in range (1,251):
    filename = str(i) + '.jpg'
    img = cv2.imread(directory_name + '/' + filename)
    img_shape = img.shape
    new_img = img[int(img_shape[0]/10*4):int(img_shape[0]/10*6), int(img_shape[1]/10*4):int(img_shape[1]/10*6)]
    resize_image = cv2.resize(new_img, (224, 224), interpolation=cv2.INTER_LINEAR)  # bilinear interpolation
    Image[i-1,:,:,:] = resize_image

# Normalization of label data
label = np.arange(2, 501, 2)
def z_score_normalize(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized_data = (data - mean) / std
    return mean,std,normalized_data
mean_label, std_label, normalized_label = z_score_normalize(label)

# Image Rotation Data Enhancement Operation
rotated_image_90, rotated_image_180, rotated_image_270 = Image,Image,Image
for i in range (1,251):
    rotated_image_90[i-1,:,:,:] = cv2.rotate(Image[i-1,:,:,:], cv2.ROTATE_90_CLOCKWISE)
    rotated_image_180[i-1,:,:,:] = cv2.rotate(Image[i-1,:,:,:], cv2.ROTATE_180)
    rotated_image_270[i-1,:,:,:] = cv2.rotate(Image[i-1,:,:,:], cv2.ROTATE_90_COUNTERCLOCKWISE)
Image_data = np.concatenate((Image, rotated_image_90, rotated_image_180, rotated_image_270), axis=0)
label_data = np.concatenate((label, label, label, label), axis=0)
label_data = torch.from_numpy(label_data)
label_data = label_data.to(torch.float32)

# TensorDataset
input = torch.from_numpy(Image_data)  # ( 1000,224,224,3 )
indices = np.arange(input.shape[0])
x_train, x_test, y_train, y_test, indices_train, indices_test = train_test_split(input, label_data, indices, test_size=0.2, random_state=84)
x_train = x_train.permute(0,3,1,2)  # ( 1000,3,224,224 )
x_test = x_test.permute(0,3,1,2)
x_train = x_train.to(torch.float32)
x_test = x_test.to(torch.float32)
train_dataset = TensorDataset(x_train,y_train)
test_dataset = TensorDataset(x_test,y_test)
predict_dataset = TensorDataset(Image_test,test_label)

# Network
class CnnNet(nn.Module):
    def __init__(self, num_classes=1, init_weights=False):
        super(CnnNet, self).__init__()
        self.features = nn.Sequential(
            # C1
            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 224, 224]  output[48, 55, 55]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]
            # C2
            nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]
            # C3
            nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
            nn.ReLU(inplace=True),
            # C4
            nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
            nn.ReLU(inplace=True),
            # C5
            nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]
        )

        self.predict = nn.Sequential(
            # FC1
            nn.Dropout(p=0.5),  # dropout rateï¼š0.5
            nn.Linear(128 * 6 * 6, 2048),
            nn.ReLU(inplace=True),
            # FC2
            nn.Dropout(p=0.5),
            nn.Linear(2048, 2048),
            nn.ReLU(inplace=True),
            # FC3
            nn.Dropout(p=0.5),
            nn.Linear(2048, 1024),
            nn.ReLU(inplace=True),
            # FC4
            nn.Dropout(p=0.5),
            nn.Linear(1024, 512),
            nn.ReLU(inplace=True),
            # FC5
            nn.Dropout(p=0.5),
            nn.Linear(512, num_classes)
        )

        if init_weights:
            self._initialize_weights()

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, start_dim=1)  # flatten layer
        x = self.predict(x)
        return x

    # weight initialize
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.001)
                nn.init.constant_(m.bias, 0)

# device
print(torch.cuda.is_available())
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# data loading
BatchSize = 16
train_dataloader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True, num_workers=0, drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=BatchSize, shuffle=False, num_workers=0, drop_last=True)
predict_dataloader = DataLoader(predict_dataset, batch_size=BatchSize, shuffle=False, num_workers=0, drop_last=True)
myModel = CnnNet(num_classes=1, init_weights=True)
myModel.to(device)

# loss function(MSE)
loss_fn = nn.MSELoss()
loss_fn.to(device)

# optimizer
learning_rate = 0.00005
optimizer = torch.optim.Adam(myModel.parameters(), lr=learning_rate)

# training epoch
epochs = 20

for epoch in range(epochs):
    print("-----------------the{}epoch training--------------------".format(epoch + 1))
    # train
    myModel.train()
    total_train_loss = 0
    for train_step, data in enumerate(train_dataloader, 0):
        images, labels = data
        images, labels = images.to(device), labels.to(device)
        outputs = torch.abs(myModel(images))
        loss = loss_fn(outputs, labels)
        # optimization model
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    print("train Loss: {:.3f}".format(total_train_loss / len(train_dataloader)))
